{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fe6d5-eec9-431d-9b09-9ec8742cc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "def extract_invention_details(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else ''\n",
    "\n",
    "    description_section = soup.find('div', class_='c_tp_description')\n",
    "    description_html = str(description_section)\n",
    "    \n",
    "    description_match = re.search(r'<p>(.*?)(?:</p>|<br/>)', description_html, re.DOTALL)\n",
    "    description = description_match.group(1).strip() if description_match else ''\n",
    "\n",
    "    background_match = re.search(r'<strong>Background:\\s*<\\/strong>(.*?)(?=<strong>|$)', description_html, re.DOTALL)\n",
    "    background = background_match.group(1).strip() if background_match else ''\n",
    "\n",
    "    background = re.sub(r'<(?:br|p|/p|/br|br/)[^>]*>', '', background)\n",
    "\n",
    "    category = \"Could not find\"\n",
    "\n",
    "    td = soup.find('td', attrs={'valign': 'top', 'style': 'width:200px;'})\n",
    "        \n",
    "    if td:\n",
    " \n",
    "        description_div = td.find('div', class_='c_tp_description')\n",
    "        \n",
    "        if description_div:\n",
    "\n",
    "            first_link = description_div.find('td').find('a')\n",
    "            \n",
    "            if first_link:\n",
    "\n",
    "                link_text = first_link.text.strip()\n",
    "                link_text = link_text[29:]\n",
    "\n",
    "                first_cat = link_text.split(' > ')[0]\n",
    "                category = first_cat\n",
    "\n",
    "    return {\n",
    "        'title': title.strip(),\n",
    "        'description': description.strip(),\n",
    "        'background': background.strip(),\n",
    "        'category': category.strip()\n",
    "    }\n",
    "\n",
    "url = 'https://inventions.arizona.edu/tech/New_Sunscreens_Based_on_Particles_Prepared_from_Higher_Aldehydes_and_Phenols'\n",
    "details = extract_invention_details(url)\n",
    "print('Title:', details['title'])\n",
    "print('\\nDescription:', details['description'])\n",
    "print('\\nBackground:', details['background'])\n",
    "print('\\nCategory:', details['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e241544-cd72-4aa3-b236-9294687afbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_on_page(page_num):\n",
    "    url = \"https://inventions.arizona.edu/searchresults.aspx?q=&type=&page=\" + str(page_num) + \"&sort=datecreated&order=desc\"\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')    \n",
    "    invention_urls = []\n",
    "    for tr in soup.find_all('tr'):\n",
    "        a_tag = tr.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "\n",
    "            href = a_tag['href']\n",
    "\n",
    "            full_url = f\"https://inventions.arizona.edu{href}\"  \n",
    "            invention_urls.append(full_url)\n",
    "    \n",
    "    return invention_urls\n",
    "\n",
    "invention_urls = get_urls_on_page(0)\n",
    "for url in invention_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2526a-2cd1-488c-bb12-79d3c238cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def create_invention_dataset(start_page, end_page):\n",
    "    all_inventions = []\n",
    "    \n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        urls = get_urls_on_page(page_num)\n",
    "        \n",
    "        for url in urls:\n",
    "            try:\n",
    "                details = extract_invention_details(url)                \n",
    "                invention_data = {\n",
    "                    'Invention': details['title'],\n",
    "                    'Description': details['description'],\n",
    "                    'Background': details['background'],\n",
    "                    'Category': details['category']\n",
    "                }\n",
    "                \n",
    "                all_inventions.append(invention_data)\n",
    "                time.sleep(0.25)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {str(e)}\")\n",
    "        \n",
    "    df = pd.DataFrame(all_inventions)\n",
    "    \n",
    "    df.to_csv('inventions_dataset1.csv', index=False)\n",
    "    print(\"Dataset saved to inventions_dataset1.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "start_page = 0\n",
    "end_page = 152 \n",
    "\n",
    "dataset1 = create_invention_dataset(start_page, end_page)\n",
    "\n",
    "print(dataset1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca9ba3-da79-4adf-b826-9c1b5a87abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('inventions_dataset.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14b66b-9e0c-404a-9bfb-84e51140ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "def get_titles_on_page(page_num):\n",
    "    url = \"https://inventions.arizona.edu/searchresults.aspx?q=&type=&page=\" + str(page_num) + \"&sort=datecreated&order=desc\"\n",
    "    response = requests.get(url, verify=False)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    titles = []\n",
    "    for tr in soup.find_all('tr'):\n",
    "        a_tag = tr.find('a')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            title = a_tag.string\n",
    "            titles.append(title)\n",
    "    \n",
    "    return titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b727e1-182d-490c-9e4f-e94b357439a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for page_num in range(0,153):\n",
    "    titles = titles + get_titles_on_page(page_num)\n",
    "print(titles[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734401b0-68ba-4cc3-8fc9-57d10a498065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(titles)\n",
    "df.to_csv('titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1729a-3124-415e-8889-17346bba9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'inventions_dataset1.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.replace('Â', '')\n",
    "        text = text.replace('ThisÂ', 'This')\n",
    "        text = re.sub(r'â€‹â€‹â€‹â€‹â€‹â€‹â€', '', text)\n",
    "        text = re.sub(r'<span style=\"font-family:null\">', '', text)\n",
    "        text = re.sub(r'</?span[^>]*>', '', text)  \n",
    "        text = text.replace('â€™', \"'\")  \n",
    "        text = text.replace('â€œ', '\"').replace('â€', '\"')\n",
    "        text = re.sub(r'â€', '', text)\n",
    "    return text\n",
    "\n",
    "df['Description'] = df['Description'].apply(clean_text)\n",
    "df['Background'] = df['Background'].apply(clean_text)\n",
    "df['Invention'] = df['Invention'].apply(clean_text)\n",
    "df['Category'] = df['Category'].apply(clean_text)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "cleaned_file_path = 'cleaned_inventions_dataset1.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52eb76-5e74-4dae-843d-043b56df493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.100d.txt')\n",
    "\n",
    "def text_to_embedding(text, embeddings_index, embedding_dim=100):\n",
    "    words = word_tokenize(text.lower())\n",
    "    valid_words = [word for word in words if word in embeddings_index]\n",
    "    if not valid_words:\n",
    "        return np.zeros(embedding_dim)\n",
    "    embedding = np.mean([embeddings_index[word] for word in valid_words], axis=0)\n",
    "    return embedding\n",
    "\n",
    "df = pd.read_csv('cleaned_inventions_dataset1.csv')\n",
    "df['text'] = df['Invention'] + \" \" + df['Description'] + \" \" + df['Background']\n",
    "df['embedding'] = df['text'].apply(lambda x: text_to_embedding(x, glove_embeddings))\n",
    "\n",
    "embedding_df = pd.DataFrame(df['embedding'].tolist())\n",
    "\n",
    "df = pd.concat([df.drop(columns=['Invention', 'Description', 'Background', 'text', 'embedding']), embedding_df], axis=1)\n",
    "X = df.drop(columns=['Category'])\n",
    "y = df['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ebfc5-c4d6-45a8-b98a-995e5921e7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
